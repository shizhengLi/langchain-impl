# LangChain å®ç°ï¼šç”Ÿäº§çº§å¤§è¯­è¨€æ¨¡å‹åº”ç”¨æ¡†æ¶

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![æµ‹è¯•](https://img.shields.io/badge/æµ‹è¯•-100%25%20é€šè¿‡-brightgreen.svg)](tests/)
[![è¦†ç›–ç‡](https://img.shields.io/badge/è¦†ç›–ç‡-95%25+-green.svg)](tests/)
[![è®¸å¯è¯](https://img.shields.io/badge/è®¸å¯è¯-MIT-yellow.svg)](LICENSE)

> ğŸš€ **ä»ç¬¬ä¸€åŸåˆ™å®ç°çš„ç”Ÿäº§çº§LangChainæ¡†æ¶**ï¼Œä¸“ä¸ºæ•™è‚²ç›®çš„å’Œä¼ä¸šçº§å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘è€Œè®¾è®¡ã€‚

> ğŸ“– **[English Version (README.md)](README.md)** - If you prefer to read in English, you can view the detailed English documentation.

## ğŸ“‹ ç›®å½•

- [ğŸ¯ é¡¹ç›®æ¦‚è¿°](#-é¡¹ç›®æ¦‚è¿°)
- [ğŸ—ï¸ ç³»ç»Ÿæ¶æ„](#ï¸-ç³»ç»Ÿæ¶æ„)
- [ğŸ”§ æ ¸å¿ƒç»„ä»¶](#-æ ¸å¿ƒç»„ä»¶)
- [ğŸ¨ è®¾è®¡æ¨¡å¼](#-è®¾è®¡æ¨¡å¼)
- [âš¡ æŠ€æœ¯æ·±åº¦è§£æ](#-æŠ€æœ¯æ·±åº¦è§£æ)
- [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [ğŸ“š APIå‚è€ƒ](#-apiå‚è€ƒ)
- [ğŸ§ª æµ‹è¯•ç­–ç•¥](#-æµ‹è¯•ç­–ç•¥)
- [ğŸ” æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
- [ğŸ› ï¸ å¼€å‘æŒ‡å—](#ï¸-å¼€å‘æŒ‡å—)
- [ğŸ“Š åŸºå‡†æµ‹è¯•](#-åŸºå‡†æµ‹è¯•)

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯LangChainæ¡†æ¶çš„**å…¨é¢ä»é›¶å®ç°**ï¼Œæ—¨åœ¨å±•ç¤ºå¯¹LLMåº”ç”¨æ¶æ„çš„æ·±åº¦ç†è§£ï¼ŒåŒæ—¶ä¿æŒç”Ÿäº§çº§ä»£ç è´¨é‡ã€‚ä¸ç®€å•çš„åŒ…è£…å®ç°ä¸åŒï¼Œæœ¬é¡¹ç›®ä»ç¬¬ä¸€åŸåˆ™æ„å»ºæ ¸å¿ƒæŠ½è±¡ã€‚

### æ ¸å¿ƒç›®æ ‡

- **ğŸ“ æ•™è‚²å“è¶Šæ€§**: å±•ç¤ºå¯¹LLMåº”ç”¨æ¨¡å¼çš„æ·±åº¦ç†è§£
- **ğŸ­ ç”Ÿäº§å°±ç»ª**: ä¼ä¸šçº§ä»£ç è´¨é‡ï¼Œ100%æµ‹è¯•è¦†ç›–ç‡
- **ğŸ”§ å¯æ‰©å±•æ¶æ„**: æ”¯æŒè‡ªå®šä¹‰ç»„ä»¶çš„æ¸…æ™°æŠ½è±¡
- **âš¡ æ€§èƒ½ä¼˜åŒ–**: å¸¦æœ‰ç¼“å­˜å’Œä¼˜åŒ–çš„é«˜æ•ˆå®ç°
- **ğŸ“š æ–‡æ¡£å®Œå–„**: åŒ…å«å®é™…ç¤ºä¾‹çš„å…¨é¢æ–‡æ¡£

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### é«˜å±‚æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        åº”ç”¨å±‚ (Application Layer)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   æ™ºèƒ½ä½“    â”‚  â”‚    é“¾      â”‚  â”‚    å·¥å…·     â”‚  â”‚    è®°å¿†     â”‚  â”‚
â”‚  â”‚ (ç¼–æ’)      â”‚  â”‚ (ç»„åˆ)      â”‚  â”‚ (æ‰§è¡Œ)      â”‚  â”‚ (çŠ¶æ€)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       å¤„ç†å±‚ (Processing Layer)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    æ£€ç´¢     â”‚  â”‚   æç¤ºè¯    â”‚  â”‚    åµŒå…¥     â”‚  â”‚   æ–‡æœ¬åˆ†å‰²  â”‚  â”‚
â”‚  â”‚ (RAGæ ¸å¿ƒ)   â”‚  â”‚ (æ¨¡æ¿)      â”‚  â”‚ (å‘é‡)      â”‚  â”‚ (åˆ†å—)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     åŸºç¡€å±‚ (Foundation Layer)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   å¤§è¯­è¨€æ¨¡å‹ â”‚  â”‚  å‘é‡å­˜å‚¨   â”‚  â”‚    åŸºç±»     â”‚  â”‚    ç±»å‹     â”‚  â”‚
â”‚  â”‚ (æ¥å£)      â”‚  â”‚ (å­˜å‚¨)      â”‚  â”‚ (æŠ½è±¡)      â”‚  â”‚ (æ¨¡å‹)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ¶æ„åŸåˆ™

1. **åˆ†å±‚æ¶æ„**: æ¸…æ™°çš„å…³æ³¨ç‚¹åˆ†ç¦»å’Œæ˜ç¡®å®šä¹‰çš„æ¥å£
2. **ä¾èµ–å€’ç½®**: é«˜å±‚æ¨¡å—ä¸ä¾èµ–ä½å±‚æ¨¡å—
3. **ç»„åˆä¼˜äºç»§æ‰¿**: çµæ´»çš„ç»„ä»¶ç»„åˆ
4. **æ¥å£éš”ç¦»**: å°è€Œä¸“æ³¨çš„æ¥å£
5. **å•ä¸€èŒè´£**: æ¯ä¸ªç»„ä»¶åªæœ‰ä¸€ä¸ªå˜åŒ–çš„ç†ç”±

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. æ£€ç´¢ç³»ç»Ÿ (RAGæ ¸å¿ƒ)

æ£€ç´¢ç³»ç»Ÿæ˜¯æœ¬å®ç°çš„**æ ¸å¿ƒäº®ç‚¹**ï¼Œå…·æœ‰å¤šç§é«˜çº§æ£€ç´¢ç­–ç•¥ï¼š

#### æ–‡æ¡£æ£€ç´¢å™¨
```python
class DocumentRetriever(BaseRetriever):
    """
    ä½¿ç”¨TF-IDFã€BM25å’ŒJaccardç›¸ä¼¼åº¦çš„ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢ã€‚

    æ ¸å¿ƒç‰¹æ€§ï¼š
    - å¸¦IDFåŠ æƒçš„è¯é¢‘åˆ†æ
    - å¸¦æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–çš„BM25è¯„åˆ†
    - å¯é…ç½®åœç”¨è¯è¿‡æ»¤
    - é«˜æ•ˆå€’æ’ç´¢å¼•ç»“æ„
    """

    def __init__(self, config: Optional[RetrievalConfig] = None):
        self._inverted_index: Dict[str, Set[str]] = defaultdict(set)
        self._document_terms: Dict[str, List[str]] = {}
        self._term_frequencies: Dict[str, Dict[str, int]] = defaultdict(dict)
        self._document_frequencies: Dict[str, int] = defaultdict(int)
        self._total_documents: int = 0
```

#### å‘é‡æ£€ç´¢å™¨
```python
class VectorRetriever(BaseRetriever):
    """
    ä½¿ç”¨å¯†é›†å‘é‡è¡¨ç¤ºçš„è¯­ä¹‰æ£€ç´¢ã€‚

    æ ¸å¿ƒç‰¹æ€§ï¼š
    - å¤šç§åµŒå…¥æ¨¡å‹æ”¯æŒ
    - ç”¨äºå¤šæ ·æ€§çš„MMRï¼ˆæœ€å¤§è¾¹ç•Œç›¸å…³æ€§ï¼‰
    - å¸¦åˆ†æ•°å½’ä¸€åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦
    - æ€§èƒ½ä¼˜åŒ–çš„åµŒå…¥ç¼“å­˜
    - å¯é…ç½®çš„ç›¸ä¼¼åº¦é˜ˆå€¼
    """

    def __init__(self,
                 embedding_model: EmbeddingModel,
                 vector_store: VectorStore,
                 config: Optional[RetrievalConfig] = None):
        self._embedding_cache: Dict[str, List[float]] = {}
        self._embedding_model = embedding_model
        self._vector_store = vector_store
```

#### é›†æˆæ£€ç´¢å™¨
```python
class EnsembleRetriever(BaseRetriever):
    """
    å¤šç§æ£€ç´¢ç­–ç•¥çš„é«˜çº§èåˆã€‚

    èåˆç­–ç•¥ï¼š
    - åŠ æƒåˆ†æ•°èåˆï¼šç›¸å…³æ€§çš„çº¿æ€§ç»„åˆ
    - æ’åèåˆï¼šåŸºäºBordaè®¡æ•°çš„æ’åèšåˆ
    - å€’æ•°æ’åèåˆï¼ˆRRFï¼‰ï¼šè¡Œä¸šæ ‡å‡†èåˆç®—æ³•
    - åŠ æƒæŠ•ç¥¨ï¼šåŸºäºä½ç½®çš„æŠ•ç¥¨æƒé‡
    """

    def __init__(self,
                 retrievers: List[BaseRetriever],
                 weights: Optional[List[float]] = None,
                 fusion_strategy: str = "weighted_score"):
        self._retrievers = retrievers
        self._weights = weights or [1.0] * len(retrievers)
        self._fusion_strategy = fusion_strategy
        self._validate_configuration()
```

### 2. å¤§è¯­è¨€æ¨¡å‹æŠ½è±¡å±‚

æ”¯æŒå¤šä¸ªLLMæä¾›å•†çš„æ¸…æ™°æŠ½è±¡ï¼š

```python
class BaseLLM(ABC):
    """
    æ‰€æœ‰LLMå®ç°çš„æŠ½è±¡åŸºç±»ã€‚

    è®¾è®¡è€ƒè™‘ï¼š
    - åŒæ­¥å’Œå¼‚æ­¥æ¥å£
    - æµå¼å“åº”æ”¯æŒ
    - Tokenä½¿ç”¨è·Ÿè¸ª
    - å¸¦é‡è¯•é€»è¾‘çš„é”™è¯¯å¤„ç†
    - å¯é…ç½®çš„æ¸©åº¦å’Œå‚æ•°
    """

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> LLMResult:
        """ç”Ÿæˆå…·æœ‰å®Œå…¨å‚æ•°æ§åˆ¶çš„å“åº”"""

    @abstractmethod
    async def agenerate(self, prompt: str, **kwargs) -> LLMResult:
        """ç”¨äºå¹¶å‘å¤„ç†çš„å¼‚æ­¥ç”Ÿæˆ"""

    def stream(self, prompt: str, **kwargs) -> Iterator[str]:
        """å®æ—¶åº”ç”¨çš„æµå¼å“åº”"""
        return self._stream_generator(prompt, **kwargs)
```

### 3. è®°å¿†ç®¡ç†ç³»ç»Ÿ

å…·æœ‰å¤šç§ç­–ç•¥çš„ç²¾å¯†è®°å¿†ç®¡ç†ï¼š

```python
class ConversationBufferMemory(BaseMemory):
    """
    å…·æœ‰å¤šç§å­˜å‚¨ç­–ç•¥çš„å…¨é¢å¯¹è¯è®°å¿†ã€‚

    ç‰¹æ€§ï¼š
    - å¯é…ç½®å¤§å°çš„æ»‘åŠ¨çª—å£
    - åŸºäºTokençš„é¢„ç®—
    - é•¿å¯¹è¯çš„è¯­ä¹‰æ‘˜è¦
    - æŒä¹…åŒ–å­˜å‚¨åç«¯
    - å¯¹è¯åˆ†æ
    """

    def __init__(self,
                 max_tokens: int = 2000,
                 strategy: str = "sliding_window",
                 storage_backend: Optional[StorageBackend] = None):
        self._max_tokens = max_tokens
        self._strategy = strategy
        self._storage = storage_backend or InMemoryStorage()
        self._conversation_analytics = ConversationAnalytics()
```

## ğŸ¨ è®¾è®¡æ¨¡å¼

### 1. ç­–ç•¥æ¨¡å¼
å¹¿æ³›ç”¨äºå¯äº’æ¢ç®—æ³•ï¼š

```python
class SearchStrategy(ABC):
    """ä¸åŒæœç´¢ç®—æ³•çš„æŠ½è±¡ç­–ç•¥"""

    @abstractmethod
    def search(self, query: str, documents: List[Document]) -> List[RetrievedDocument]:
        pass

class TFIDFStrategy(SearchStrategy):
    def search(self, query: str, documents: List[Document]) -> List[RetrievedDocument]:
        # TF-IDFå®ç°
        pass

class BM25Strategy(SearchStrategy):
    def search(self, query: str, documents: List[Document]) -> List[RetrievedDocument]:
        # å¸¦k1å’Œbå‚æ•°çš„BM25å®ç°
        pass
```

### 2. å·¥å‚æ¨¡å¼
ç”¨äºç»„ä»¶åˆ›å»ºå’Œé…ç½®ï¼š

```python
class RetrieverFactory:
    """åˆ›å»ºä¸åŒç±»å‹æ£€ç´¢å™¨çš„å·¥å‚"""

    @staticmethod
    def create_retriever(retriever_type: str, **kwargs) -> BaseRetriever:
        if retriever_type == "document":
            return DocumentRetriever(**kwargs)
        elif retriever_type == "vector":
            return VectorRetriever(**kwargs)
        elif retriever_type == "ensemble":
            return EnsembleRetriever(**kwargs)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ£€ç´¢å™¨ç±»å‹: {retriever_type}")
```

### 3. è§‚å¯Ÿè€…æ¨¡å¼
ç”¨äºæ—¥å¿—è®°å½•å’Œç›‘æ§ï¼š

```python
class RetrieverObserver(ABC):
    """æ£€ç´¢äº‹ä»¶çš„è§‚å¯Ÿè€…æ¥å£"""

    @abstractmethod
    def on_retrieval_start(self, query: str, config: RetrievalConfig):
        pass

    @abstractmethod
    def on_retrieval_complete(self, result: RetrievalResult):
        pass

class PerformanceObserver(RetrieverObserver):
    """è·Ÿè¸ªæ€§èƒ½æŒ‡æ ‡çš„è§‚å¯Ÿè€…"""

    def on_retrieval_start(self, query: str, config: RetrievalConfig):
        self._start_time = time.time()

    def on_retrieval_complete(self, result: RetrievalResult):
        duration = time.time() - self._start_time
        self._metrics.record_retrieval(duration, len(result.documents))
```

### 4. æ¨¡æ¿æ–¹æ³•æ¨¡å¼
ç”¨äºé€šç”¨å¤„ç†æµæ°´çº¿ï¼š

```python
class BaseProcessor(ABC):
    """å¤„ç†æµæ°´çº¿çš„æ¨¡æ¿æ–¹æ³•æ¨¡å¼"""

    def process(self, input_data: Any) -> Any:
        # å®šä¹‰ç®—æ³•ç»“æ„çš„æ¨¡æ¿æ–¹æ³•
        validated_data = self.validate_input(input_data)
        processed_data = self.process_core(validated_data)
        return self.format_output(processed_data)

    @abstractmethod
    def process_core(self, validated_data: Any) -> Any:
        pass

    def validate_input(self, input_data: Any) -> Any:
        # é€šç”¨éªŒè¯é€»è¾‘
        return input_data

    def format_output(self, processed_data: Any) -> Any:
        # é€šç”¨æ ¼å¼åŒ–é€»è¾‘
        return processed_data
```

### 5. è´£ä»»é“¾æ¨¡å¼
ç”¨äºå¤„ç†æµæ°´çº¿ï¼š

```python
class ProcessingStep(ABC):
    """å¤„ç†æ­¥éª¤çš„è´£ä»»é“¾"""

    def __init__(self):
        self._next_step: Optional[ProcessingStep] = None

    def set_next(self, step: 'ProcessingStep') -> 'ProcessingStep':
        self._next_step = step
        return step

    @abstractmethod
    def handle(self, request: ProcessingRequest) -> ProcessingResponse:
        response = self.process(request)
        if self._next_step and not response.is_complete:
            response = self._next_step.handle(request)
        return response
```

## âš¡ æŠ€æœ¯æ·±åº¦è§£æ

### 1. é«˜çº§æ£€ç´¢ç®—æ³•

#### BM25å®ç°
```python
def _calculate_bm25_score(self, query_terms: List[str], doc_id: str) -> float:
    """
    å¸¦k1å’Œbå‚æ•°çš„BM25è¯„åˆ†ç®—æ³•ã€‚

    BM25(q,d) = Î£ IDF(qi) * (f(qi,d) * (k1+1)) / (f(qi,d) + k1 * (1-b+b*|d|/avgdl))

    å…¶ä¸­ï¼š
    - f(qi,d): è¯é¡¹qiåœ¨æ–‡æ¡£dä¸­çš„é¢‘ç‡
    - |d|: æ–‡æ¡£dçš„é•¿åº¦ï¼ˆè¯æ•°ï¼‰
    - avgdl: é›†åˆä¸­æ–‡æ¡£çš„å¹³å‡é•¿åº¦
    - k1: æ§åˆ¶è¯é¢‘é¥±å’Œåº¦ï¼ˆé€šå¸¸1.2-2.0ï¼‰
    - b: æ§åˆ¶æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ï¼ˆé€šå¸¸0.75ï¼‰
    """
    k1 = 1.2  # è¯é¢‘é¥±å’Œå‚æ•°
    b = 0.75  # é•¿åº¦å½’ä¸€åŒ–å‚æ•°

    score = 0.0
    doc_length = len(self._document_terms[doc_id])
    avg_doc_length = self._get_average_document_length()

    for term in query_terms:
        if term in self._term_frequencies and doc_id in self._term_frequencies[term]:
            tf = self._term_frequencies[term][doc_id]
            idf = self._calculate_idf(term)

            # BM25å…¬å¼
            numerator = tf * (k1 + 1)
            denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))
            score += idf * (numerator / denominator)

    return score
```

#### MMRï¼ˆæœ€å¤§è¾¹ç•Œç›¸å…³æ€§ï¼‰
```python
def _mmr_rerank(self,
                candidates: List[RetrievedDocument],
                query_embedding: List[float],
                lambda_param: float) -> List[RetrievedDocument]:
    """
    å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§çš„æœ€å¤§è¾¹ç•Œç›¸å…³æ€§ã€‚

    MMR = arg max_{Di âˆˆ R\Q} [ Î» * sim(Di, Q) - (1-Î») * max_{Dj âˆˆ Q} sim(Di, Dj) ]

    å…¶ä¸­ï¼š
    - Î»: æ§åˆ¶ç›¸å…³æ€§å’Œå¤šæ ·æ€§ä¹‹é—´çš„å¹³è¡¡
    - sim(Di, Q): æ–‡æ¡£Diå’ŒæŸ¥è¯¢Qä¹‹é—´çš„ç›¸ä¼¼åº¦
    - sim(Di, Dj): æ–‡æ¡£Diå’ŒDjä¹‹é—´çš„ç›¸ä¼¼åº¦
    """
    if not candidates:
        return []

    selected = []
    remaining = candidates.copy()

    # é¦–å…ˆé€‰æ‹©æœ€ç›¸å…³çš„æ–‡æ¡£
    first_doc = max(remaining, key=lambda d: d.relevance_score)
    selected.append(first_doc)
    remaining.remove(first_doc)

    while remaining and len(selected) < self.config.top_k:
        best_doc = None
        best_score = float('-inf')

        for doc in remaining:
            # ç›¸å…³æ€§ç»„ä»¶
            relevance = doc.relevance_score

            # å¤šæ ·æ€§ç»„ä»¶ï¼ˆä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼‰
            max_similarity = 0.0
            doc_embedding = self._get_document_embedding(doc.id)

            for selected_doc in selected:
                selected_embedding = self._get_document_embedding(selected_doc.id)
                similarity = self._cosine_similarity(doc_embedding, selected_embedding)
                max_similarity = max(max_similarity, similarity)

            # MMRåˆ†æ•°
            mmr_score = lambda_param * relevance - (1 - lambda_param) * max_similarity

            if mmr_score > best_score:
                best_score = mmr_score
                best_doc = doc

        if best_doc:
            selected.append(best_doc)
            remaining.remove(best_doc)

    return selected
```

### 2. å‘é‡æ“ä½œå’Œä¼˜åŒ–

#### é«˜æ•ˆå‘é‡ç›¸ä¼¼åº¦
```python
class VectorOperations:
    """ä½¿ç”¨NumPyä¼˜åŒ–çš„é«˜æ€§èƒ½å‘é‡æ“ä½œ"""

    @staticmethod
    @lru_cache(maxsize=1024)
    def cosine_similarity_cached(vec1_id: str, vec2_id: str,
                               vector_store: 'VectorStore') -> float:
        """ç¼“å­˜çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—"""
        vec1 = vector_store.get_vector(vec1_id)
        vec2 = vector_store.get_vector(vec2_id)
        return VectorOperations.cosine_similarity(vec1, vec2)

    @staticmethod
    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """ä½¿ç”¨NumPyä¼˜åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0

        # è½¬æ¢ä¸ºNumPyæ•°ç»„è¿›è¡Œå‘é‡åŒ–æ“ä½œ
        a = np.array(vec1, dtype=np.float32)
        b = np.array(vec2, dtype=np.float32)

        # å‘é‡åŒ–è®¡ç®—
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)

        if norm_a == 0 or norm_b == 0:
            return 0.0

        return float(dot_product / (norm_a * norm_b))

    @staticmethod
    def batch_cosine_similarity(query_vec: List[float],
                               doc_vectors: List[List[float]]) -> List[float]:
        """ä½™å¼¦ç›¸ä¼¼åº¦çš„æ‰¹é‡è®¡ç®—"""
        if not doc_vectors:
            return []

        query_array = np.array(query_vec, dtype=np.float32)
        doc_matrix = np.array(doc_vectors, dtype=np.float32)

        # å‘é‡åŒ–æ‰¹é‡è®¡ç®—
        dot_products = np.dot(doc_matrix, query_array)
        doc_norms = np.linalg.norm(doc_matrix, axis=1)
        query_norm = np.linalg.norm(query_array)

        # å¤„ç†é›¶å‘é‡
        valid_mask = (doc_norms > 0) & (query_norm > 0)
        similarities = np.zeros(len(doc_vectors))
        similarities[valid_mask] = dot_products[valid_mask] / (doc_norms[valid_mask] * query_norm)

        return similarities.tolist()
```

### 3. å†…å­˜ç®¡ç†å’Œç¼“å­˜

#### å¤šçº§ç¼“å­˜ç­–ç•¥
```python
class MultiLevelCache:
    """
    å…·æœ‰L1ï¼ˆå†…å­˜ï¼‰ã€L2ï¼ˆç£ç›˜ï¼‰å’ŒL3ï¼ˆåˆ†å¸ƒå¼ï¼‰å±‚çº§çš„åˆ†å±‚ç¼“å­˜ç³»ç»Ÿã€‚
    """

    def __init__(self,
                 l1_size: int = 1000,
                 l2_size: int = 10000,
                 l3_backend: Optional[CacheBackend] = None):
        self._l1_cache = LRUCache(maxsize=l1_size)  # çƒ­æ•°æ®
        self._l2_cache = LRUCache(maxsize=l2_size)  # æ¸©æ•°æ®
        self._l3_backend = l3_backend  # å†·æ•°æ®

    async def get(self, key: str) -> Optional[Any]:
        """ç¼“å­˜å±‚æ¬¡éå†è·å–å€¼"""
        # L1ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        if key in self._l1_cache:
            return self._l1_cache[key]

        # L2ç¼“å­˜
        if key in self._l2_cache:
            value = self._l2_cache[key]
            self._l1_cache[key] = value  # æå‡åˆ°L1
            return value

        # L3ç¼“å­˜ï¼ˆæœ€æ…¢ï¼‰
        if self._l3_backend:
            value = await self._l3_backend.get(key)
            if value is not None:
                self._l2_cache[key] = value  # æå‡åˆ°L2
                return value

        return None

    async def set(self, key: str, value: Any, ttl: Optional[int] = None):
        """ç¼“å­˜ä¼ æ’­è®¾ç½®å€¼"""
        self._l1_cache[key] = value
        self._l2_cache[key] = value

        if self._l3_backend:
            await self._l3_backend.set(key, value, ttl)
```

### 4. å¹¶å‘å’Œå¼‚æ­¥å¤„ç†

#### å¼‚æ­¥æ‰¹å¤„ç†
```python
class BatchProcessor:
    """ä½¿ç”¨asyncioçš„é«˜æ€§èƒ½æ‰¹å¤„ç†"""

    def __init__(self, batch_size: int = 32, max_concurrency: int = 10):
        self.batch_size = batch_size
        self.semaphore = asyncio.Semaphore(max_concurrency)

    async def process_documents(self,
                              documents: List[Document],
                              processor: Callable[[Document], Awaitable[Any]]) -> List[Any]:
        """å—æ§å¹¶å‘æ‰¹å¤„ç†æ–‡æ¡£"""
        results = []

        # åˆ†å‰²æˆæ‰¹æ¬¡
        batches = [documents[i:i + self.batch_size]
                  for i in range(0, len(documents), self.batch_size)]

        # å¹¶å‘å¤„ç†æ‰¹æ¬¡
        async def process_batch(batch: List[Document]) -> List[Any]:
            async with self.semaphore:
                tasks = [processor(doc) for doc in batch]
                return await asyncio.gather(*tasks, return_exceptions=True)

        # æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
        batch_results = await asyncio.gather(*[process_batch(batch) for batch in batches])

        # å±•å¹³ç»“æœ
        for batch_result in batch_results:
            for result in batch_result:
                if not isinstance(result, Exception):
                    results.append(result)
                else:
                    logger.error(f"å¤„ç†é”™è¯¯: {result}")

        return results
```

### 5. ç±»ç³»ç»Ÿå’ŒéªŒè¯

#### å¸¦è‡ªå®šä¹‰éªŒè¯å™¨çš„Pydanticæ¨¡å‹
```python
from pydantic import BaseModel, Field, validator, root_validator
from typing import List, Dict, Any, Optional, Union
import numpy as np

class RetrievalConfig(BaseModel):
    """
    å¸¦éªŒè¯çš„å…¨é¢æ£€ç´¢é…ç½®ã€‚
    """

    # æ ¸å¿ƒå‚æ•°
    top_k: int = Field(default=5, ge=1, le=100, description="æ£€ç´¢çš„æ–‡æ¡£æ•°é‡")
    score_threshold: Optional[float] = Field(default=None, ge=0.0, le=1.0,
                                            description="æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°")
    search_type: str = Field(default="similarity",
                           regex="^(similarity|mmr|hybrid|tfidf|bm25)$",
                           description="æœç´¢ç®—æ³•ç±»å‹")

    # MMRå‚æ•°
    mmr_lambda: float = Field(default=0.5, ge=0.0, le=1.0,
                             description="MMRå¤šæ ·æ€§å‚æ•°")
    fetch_k: int = Field(default=20, ge=1, le=1000,
                        description="MMRå€™é€‰æ–‡æ¡£æ•°é‡")

    # æ€§èƒ½å‚æ•°
    enable_caching: bool = Field(default=True, description="å¯ç”¨ç»“æœç¼“å­˜")
    cache_ttl: Optional[float] = Field(default=300.0, gt=0,
                                       description="ç¼“å­˜TTLï¼ˆç§’ï¼‰")
    batch_size: int = Field(default=32, ge=1, le=256,
                           description="æ‰¹å¤„ç†å¤§å°")

    # è¿‡æ»¤å‚æ•°
    filter_dict: Dict[str, Any] = Field(default_factory=dict,
                                        description="å…ƒæ•°æ®è¿‡æ»¤å™¨")

    @validator('top_k')
    def validate_top_k(cls, v):
        if v <= 0:
            raise ValueError('top_kå¿…é¡»ä¸ºæ­£æ•°')
        return v

    @validator('mmr_lambda')
    def validate_mmr_lambda(cls, v):
        if not 0 <= v <= 1:
            raise ValueError('mmr_lambdaå¿…é¡»åœ¨0å’Œ1ä¹‹é—´')
        return v

    @root_validator
    def validate_consistency(cls, values):
        """éªŒè¯é…ç½®ä¸€è‡´æ€§"""
        search_type = values.get('search_type', '')
        mmr_lambda = values.get('mmr_lambda', 0.5)

        if search_type == 'mmr' and not (0 < mmr_lambda < 1):
            raise ValueError('MMRæœç´¢æ—¶mmr_lambdaå¿…é¡»åœ¨0å’Œ1ä¹‹é—´')

        return values
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-username/langchain-impl.git
cd langchain-impl

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œæµ‹è¯•éªŒè¯å®‰è£…
pytest
```

### åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

#### 1. ç®€å•æ–‡æ¡£æ£€ç´¢
```python
from my_langchain.retrieval import DocumentRetriever, Document, RetrievalConfig

# åˆ›å»ºå¸¦è‡ªå®šä¹‰é…ç½®çš„æ£€ç´¢å™¨
config = RetrievalConfig(
    top_k=5,
    search_type="bm25",
    score_threshold=0.3
)
retriever = DocumentRetriever(config=config)

# æ·»åŠ æ–‡æ¡£
documents = [
    Document(
        content="Pythonæ˜¯ä¸€ç§å…·æœ‰åŠ¨æ€è¯­ä¹‰çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚",
        metadata={"source": "wikipedia", "category": "programming"}
    ),
    Document(
        content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ã€‚",
        metadata={"source": "textbook", "category": "ai"}
    ),
    Document(
        content="æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚",
        metadata={"source": "research", "category": "ai"}
    )
]

doc_ids = retriever.add_documents(documents)
print(f"å·²æ·»åŠ  {len(doc_ids)} ä¸ªæ–‡æ¡£")

# æ‰§è¡Œæ£€ç´¢
result = retriever.retrieve("ç¥ç»ç½‘ç»œ")
print(f"åœ¨ {result.search_time:.4f}s å†…æ‰¾åˆ° {len(result.documents)} ä¸ªæ–‡æ¡£")

for i, doc in enumerate(result.documents, 1):
    print(f"{i}. åˆ†æ•°: {doc.relevance_score:.3f}")
    print(f"   å†…å®¹: {doc.content}")
    print(f"   æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
```

#### 2. å¸¦MMRçš„é«˜çº§å‘é‡æ£€ç´¢
```python
from my_langchain.retrieval import VectorRetriever
from my_langchain.embeddings import MockEmbedding
from my_langchain.vectorstores import InMemoryVectorStore
from my_langchain.vectorstores.types import VectorStoreConfig

# åˆ›å»ºå¸¦é…ç½®çš„å‘é‡å­˜å‚¨
vector_config = VectorStoreConfig(
    dimension=384,
    metric="cosine"
)
vector_store = InMemoryVectorStore(config=vector_config)

# åˆ›å»ºåµŒå…¥æ¨¡å‹
embedding_model = MockEmbedding(embedding_dimension=384)

# åˆ›å»ºå¸¦MMRçš„å‘é‡æ£€ç´¢å™¨
config = RetrievalConfig(
    search_type="mmr",
    mmr_lambda=0.7,  # æ›´é«˜å¤šæ ·æ€§
    top_k=3
)
retriever = VectorRetriever(
    embedding_model=embedding_model,
    vector_store=vector_store,
    config=config
)

# æ·»åŠ æ–‡æ¡£ï¼ˆå°†è‡ªåŠ¨åµŒå…¥ï¼‰
retriever.add_documents(documents)

# æ‰§è¡Œå¸¦å¤šæ ·æ€§çš„è¯­ä¹‰æ£€ç´¢
result = retriever.retrieve("äººå·¥æ™ºèƒ½å’Œç¥ç»ç½‘ç»œ")
print(f"æ£€ç´¢æ–¹æ³•: {result.retrieval_method}")
print(f"MMRå¤šæ ·æ€§ç»“æœ (Î»={config.mmr_lambda}):")

for i, doc in enumerate(result.documents, 1):
    print(f"{i}. åˆ†æ•°: {doc.relevance_score:.3f}")
    print(f"   å†…å®¹: {doc.content}")
    if doc.additional_info:
        print(f"   é™„åŠ ä¿¡æ¯: {doc.additional_info}")
```

#### 3. å¤šç­–ç•¥é›†æˆæ£€ç´¢
```python
from my_langchain.retrieval import EnsembleRetriever

# åˆ›å»ºå¤šä¸ªæ£€ç´¢å™¨
doc_retriever = DocumentRetriever(config=RetrievalConfig(search_type="bm25"))
vector_retriever = VectorRetriever(
    embedding_model=embedding_model,
    vector_store=vector_store,
    config=RetrievalConfig(search_type="similarity")
)

# å‘æ‰€æœ‰æ£€ç´¢å™¨æ·»åŠ æ–‡æ¡£
for retriever in [doc_retriever, vector_retriever]:
    retriever.add_documents(documents)

# åˆ›å»ºå¸¦è‡ªå®šä¹‰èåˆç­–ç•¥çš„é›†æˆæ£€ç´¢å™¨
ensemble = EnsembleRetriever(
    retrievers=[doc_retriever, vector_retriever],
    weights=[0.3, 0.7],  # åå‘å‘é‡æ£€ç´¢
    fusion_strategy="reciprocal_rank",
    config=RetrievalConfig(top_k=5)
)

# æ‰§è¡Œé›†æˆæ£€ç´¢
result = ensemble.retrieve("ç¼–ç¨‹è¯­è¨€")

# æ¯”è¾ƒå„ä¸ªæ£€ç´¢å™¨çš„æ€§èƒ½
comparison = ensemble.compare_retrievers("ç¼–ç¨‹è¯­è¨€")
print("æ£€ç´¢å™¨æ¯”è¾ƒ:")
for name, comp_result in comparison.items():
    print(f"{name}: {len(comp_result.documents)} ä¸ªç»“æœ, "
          f"å¹³å‡åˆ†æ•°: {comp_result.get_average_score():.3f}")

print(f"\né›†æˆç»“æœ: {len(result.documents)} ä¸ªæ–‡æ¡£")
for i, doc in enumerate(result.documents, 1):
    source_info = doc.additional_info.get("source_retrievers", [])
    print(f"{i}. åˆ†æ•°: {doc.relevance_score:.3f} (æ¥æº: {', '.join(source_info)})")
    print(f"   å†…å®¹: {doc.content}")
```

#### 4. å¸¦è®°å¿†çš„é“¾ç»„åˆ
```python
from my_langchain.chains import LLMChain
from my_langchain.prompts import PromptTemplate
from my_langchain.memory import ConversationBufferMemory
from my_langchain.llms import MockLLM

# åˆ›å»ºå¸¦å¯¹è¯å†å²çš„è®°å¿†
memory = ConversationBufferMemory(
    max_tokens=2000,
    strategy="sliding_window"
)

# åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    template="""ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚åŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡: {context}

å¯¹è¯å†å²:
{history}

é—®é¢˜: {question}

å›ç­”:""",
    input_variables=["context", "history", "question"]
)

# åˆ›å»ºLLM
llm = MockLLM(responses=[
    "åŸºäºä¸Šä¸‹æ–‡ï¼ŒPythonç¡®å®æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ã€‚",
    "å†å²æ˜¾ç¤ºæˆ‘ä»¬æ­£åœ¨è®¨è®ºç¼–ç¨‹è¯­è¨€ã€‚",
    "æ ¹æ®æ–‡æ¡£ï¼Œç¥ç»ç½‘ç»œç”¨äºæ·±åº¦å­¦ä¹ ã€‚"
])

# åˆ›å»ºé“¾
chain = LLMChain(
    llm=llm,
    prompt=prompt,
    memory=memory
)

# ä½¿ç”¨æ£€ç´¢ä¸Šä¸‹æ–‡æ‰§è¡Œé“¾
context = "\n".join([doc.content for doc in result.documents[:2]])
question = "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"

response = chain.run(
    context=context,
    question=question
)

print(f"é—®é¢˜: {question}")
print(f"å›ç­”: {response}")
```

## ğŸ“š APIå‚è€ƒ

### æ£€ç´¢ç³»ç»ŸAPI

#### DocumentRetriever
```python
class DocumentRetriever(BaseRetriever):
    """ä½¿ç”¨TF-IDFå’ŒBM25çš„ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢"""

    def __init__(self, config: Optional[RetrievalConfig] = None):
        """ä½¿ç”¨å¯é€‰é…ç½®åˆå§‹åŒ–"""

    def add_documents(self, documents: List[Document]) -> List[str]:
        """æ·»åŠ æ–‡æ¡£å¹¶è¿”å›æ–‡æ¡£ID"""

    def retrieve(self, query: str) -> RetrievalResult:
        """ä¸ºæŸ¥è¯¢æ£€ç´¢æ–‡æ¡£"""

    def get_term_statistics(self) -> Dict[str, Any]:
        """è·å–è¯é¢‘å’Œæ–‡æ¡£ç»Ÿè®¡"""

    def search_by_term(self, term: str) -> List[str]:
        """æŸ¥æ‰¾åŒ…å«ç‰¹å®šè¯é¡¹çš„æ–‡æ¡£"""
```

#### VectorRetriever
```python
class VectorRetriever(BaseRetriever):
    """ä½¿ç”¨å‘é‡åµŒå…¥çš„è¯­ä¹‰æ£€ç´¢"""

    def __init__(self,
                 embedding_model: EmbeddingModel,
                 vector_store: VectorStore,
                 config: Optional[RetrievalConfig] = None):
        """ä½¿ç”¨åµŒå…¥æ¨¡å‹å’Œå‘é‡å­˜å‚¨åˆå§‹åŒ–"""

    def add_documents(self, documents: List[Document]) -> List[str]:
        """æ·»åŠ æ–‡æ¡£å¹¶è‡ªåŠ¨åµŒå…¥"""

    def retrieve(self, query: str) -> RetrievalResult:
        """ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢"""

    def get_embedding_stats(self) -> Dict[str, Any]:
        """è·å–åµŒå…¥å’Œç¼“å­˜ç»Ÿè®¡"""

    def clear_cache(self):
        """æ¸…ç©ºåµŒå…¥ç¼“å­˜"""
```

#### EnsembleRetriever
```python
class EnsembleRetriever(BaseRetriever):
    """å¤šç§æ£€ç´¢ç­–ç•¥çš„èåˆ"""

    def __init__(self,
                 retrievers: List[BaseRetriever],
                 weights: Optional[List[float]] = None,
                 fusion_strategy: str = "weighted_score"):
        """ä½¿ç”¨æ£€ç´¢å™¨å’Œèåˆç­–ç•¥åˆå§‹åŒ–"""

    def compare_retrievers(self, query: str) -> Dict[str, RetrievalResult]:
        """æ¯”è¾ƒæ‰€æœ‰æ£€ç´¢å™¨çš„ç»“æœ"""

    def get_ensemble_stats(self) -> Dict[str, Any]:
        """è·å–é›†æˆç»Ÿè®¡å’Œæ€§èƒ½æŒ‡æ ‡"""

    def set_fusion_strategy(self, strategy: str):
        """è¿è¡Œæ—¶æ›´æ”¹èåˆç­–ç•¥"""
```

### æ•°æ®æ¨¡å‹

#### Document
```python
class Document(BaseModel):
    """å¸¦å†…å®¹å’Œå…ƒæ•°æ®çš„æ ¸å¿ƒæ–‡æ¡£æ¨¡å‹"""
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))

    def get_text_snippet(self, max_length: int = 100) -> str:
        """è·å–æ–‡æ¡£é¢„è§ˆ"""

    def matches_filter(self, filter_dict: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦åŒ¹é…å…ƒæ•°æ®è¿‡æ»¤å™¨"""
```

#### RetrievalResult
```python
class RetrievalResult(BaseModel):
    """å¸¦å…ƒæ•°æ®çš„å…¨é¢æ£€ç´¢ç»“æœ"""
    documents: List[RetrievedDocument]
    query: str
    total_results: int
    search_time: float
    retrieval_method: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

    def get_top_k(self, k: int) -> List[RetrievedDocument]:
        """è·å–å‰kä¸ªç»“æœ"""

    def get_average_score(self) -> float:
        """è®¡ç®—å¹³å‡ç›¸å…³æ€§åˆ†æ•°"""

    def filter_by_metadata(self, key: str, value: Any) -> 'RetrievalResult':
        """æŒ‰å…ƒæ•°æ®è¿‡æ»¤ç»“æœ"""
```

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### æµ‹è¯•æ¶æ„

é¡¹ç›®é‡‡ç”¨åŒ…å«å¤šç§æµ‹è¯•ç±»å‹çš„å…¨é¢æµ‹è¯•ç­–ç•¥ï¼š

```python
# ä¸ªåˆ«ç»„ä»¶çš„å•å…ƒæµ‹è¯•
class TestDocumentRetriever:
    def test_add_documents(self):
        """æµ‹è¯•å¸¦éªŒè¯çš„æ–‡æ¡£æ·»åŠ """

    def test_retrieve_with_filters(self):
        """æµ‹è¯•å¸¦å…ƒæ•°æ®è¿‡æ»¤çš„æ£€ç´¢"""

    def test_term_statistics(self):
        """æµ‹è¯•è¯é¢‘è®¡ç®—"""

# ç»„ä»¶äº¤äº’çš„é›†æˆæµ‹è¯•
class TestEnsembleRetrieval:
    def test_multiple_retrievers(self):
        """æµ‹è¯•ä¸åŒæ£€ç´¢å™¨ç±»å‹çš„é›†æˆ"""

    def test_fusion_strategies(self):
        """æµ‹è¯•ä¸åŒèåˆç®—æ³•"""

# æ€§èƒ½æµ‹è¯•
class TestPerformance:
    def test_large_scale_retrieval(self):
        """æµ‹è¯•å¤§æ•°æ®é›†çš„æ€§èƒ½"""

    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
```

### æµ‹è¯•è¦†ç›–ç‡

- **å•å…ƒæµ‹è¯•**: æ‰€æœ‰æ¨¡å—90%+è¡Œè¦†ç›–ç‡
- **é›†æˆæµ‹è¯•**: ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•
- **æ€§èƒ½æµ‹è¯•**: åŸºå‡†æµ‹è¯•å’Œå›å½’æµ‹è¯•
- **å±æ€§æµ‹è¯•**: åŸºäºHypothesisçš„è¾¹ç•Œæ¡ä»¶æµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œå¸¦è¦†ç›–ç‡çš„æµ‹è¯•
pytest --cov=my_langchain --cov-report=html

# è¿è¡Œç‰¹å®šæµ‹è¯•ç±»åˆ«
pytest -m unit      # ä»…å•å…ƒæµ‹è¯•
pytest -m integration # ä»…é›†æˆæµ‹è¯•
pytest -m slow      # ä»…æ€§èƒ½æµ‹è¯•

# è¿è¡Œå¸¦ç‰¹å®šæ ‡è®°çš„æµ‹è¯•
pytest -k "retrieval"  # æ£€ç´¢ç›¸å…³æµ‹è¯•
pytest -k "ensemble"   # é›†æˆæ–¹æ³•ç›¸å…³æµ‹è¯•
```

## ğŸ” æ€§èƒ½ä¼˜åŒ–

### 1. ç¼“å­˜ç­–ç•¥

#### å¤šçº§ç¼“å­˜
```python
# L1: çƒ­æ•°æ®çš„å†…å­˜ç¼“å­˜
@lru_cache(maxsize=1000)
def cached_embedding(text: str) -> List[float]:
    return embedding_model.embed(text)

# L2: æ¸©æ•°æ®çš„ç£ç›˜ç¼“å­˜
class DiskCache:
    def __init__(self, cache_dir: str):
        self.cache_dir = Path(cache_dir)

    def get(self, key: str) -> Optional[Any]:
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
```

### 2. æ‰¹å¤„ç†

#### å‘é‡åŒ–æ“ä½œ
```python
def batch_cosine_similarity(query_vec: np.ndarray,
                           doc_vectors: np.ndarray) -> np.ndarray:
    """å‘é‡åŒ–ç›¸ä¼¼åº¦è®¡ç®—"""
    # ä¸€æ¬¡æ€§å½’ä¸€åŒ–å‘é‡
    query_norm = np.linalg.norm(query_vec)
    doc_norms = np.linalg.norm(doc_vectors, axis=1, keepdims=True)

    # å‘é‡åŒ–ç‚¹ç§¯
    similarities = np.dot(doc_vectors, query_vec) / (doc_norms.flatten() * query_norm)
    return similarities
```

### 3. å†…å­˜ç®¡ç†

#### æ‡’åŠ è½½
```python
class LazyDocumentLoader:
    """ä»…åœ¨éœ€è¦æ—¶åŠ è½½æ–‡æ¡£"""

    def __init__(self, document_paths: List[str]):
        self.document_paths = document_paths
        self._loaded_documents: Dict[str, Document] = {}

    def get_document(self, doc_id: str) -> Document:
        if doc_id not in self._loaded_documents:
            self._loaded_documents[doc_id] = self._load_from_disk(doc_id)
        return self._loaded_documents[doc_id]
```

### 4. å¹¶å‘å¤„ç†

#### å¼‚æ­¥å®ç°
```python
async def parallel_retrieval(query: str,
                            retrievers: List[BaseRetriever]) -> List[RetrievalResult]:
    """å¹¶è¡Œè¿è¡Œæ£€ç´¢"""
    tasks = [retriever.retrieve(query) for retriever in retrievers]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return [r for r in results if isinstance(r, RetrievalResult)]
```

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### ä»£ç é£æ ¼å’Œæ ‡å‡†

é¡¹ç›®éµå¾ªä¸¥æ ¼çš„ä»£ç è´¨é‡æ ‡å‡†ï¼š

```python
# æ‰€æœ‰å…¬å…±APIçš„ç±»å‹æç¤º
def process_documents(documents: List[Document]) -> List[str]:
    """å¤„ç†æ–‡æ¡£å¹¶è¿”å›ID"""

# å…¨é¢çš„æ–‡æ¡£å­—ç¬¦ä¸²
class ExampleClass:
    """
    ç±»çš„ç®€è¦æè¿°ã€‚

    è·¨è¶Šå¤šè¡Œçš„è¯¦ç»†æè¿°ï¼ŒåŒ…å«ç‰¹å®šè¡Œä¸ºè¯´æ˜ã€‚

    å±æ€§:
        attribute1: attribute1çš„æè¿°
        attribute2: attribute2çš„æè¿°

    ç¤ºä¾‹:
        >>> obj = ExampleClass()
        >>> result = obj.method()
        >>> print(result)
    """

    def method(self) -> str:
        """å¸¦è¿”å›ç±»å‹çš„æ–¹æ³•æè¿°"""
        return "result"
```

### è´¡çŒ®æŒ‡å—

1. **ä»£ç è´¨é‡**: æ‰€æœ‰ä»£ç å¿…é¡»é€šè¿‡lintingå’Œç±»å‹æ£€æŸ¥
2. **æµ‹è¯•**: æ–°åŠŸèƒ½å¿…é¡»åŒ…å«å…¨é¢æµ‹è¯•
3. **æ–‡æ¡£**: å…¬å…±APIå¿…é¡»æœ‰å®Œæ•´æ–‡æ¡£
4. **æ€§èƒ½**: è€ƒè™‘æ›´æ”¹çš„æ€§èƒ½å½±å“

### å¼€å‘å·¥ä½œæµ

```bash
# è®¾ç½®å¼€å‘ç¯å¢ƒ
git clone <repository>
cd langchain-impl
python -m venv venv
source venv/bin/activate
pip install -r requirements-dev.txt

# å®‰è£…pre-commité’©å­
pre-commit install

# è¿è¡Œè´¨é‡æ£€æŸ¥
black .
isort .
flake8 .
mypy my_langchain/
pytest
```

## ğŸ“Š åŸºå‡†æµ‹è¯•

### æ£€ç´¢æ€§èƒ½

| æ£€ç´¢å™¨ç±»å‹ | æ•°æ®é›†å¤§å° | å¹³å‡æŸ¥è¯¢æ—¶é—´ | Precision@10 | Recall@100 |
|-----------|-----------|-------------|-------------|-----------|
| æ–‡æ¡£æ£€ç´¢å™¨ | 10Kæ–‡æ¡£ | 15ms | 0.75 | 0.82 |
| å‘é‡æ£€ç´¢å™¨ | 10Kæ–‡æ¡£ | 45ms | 0.82 | 0.88 |
| é›†æˆæ£€ç´¢å™¨ | 10Kæ–‡æ¡£ | 65ms | 0.85 | 0.91 |

### å†…å­˜ä½¿ç”¨

| ç»„ä»¶ | å†…å­˜ä½¿ç”¨ | ç¼“å­˜å¤§å° | è¯´æ˜ |
|------|----------|----------|------|
| æ–‡æ¡£æ£€ç´¢å™¨ | 50MB | N/A | å€’æ’ç´¢å¼• |
| å‘é‡æ£€ç´¢å™¨ | 200MB | 100MB | åµŒå…¥ + å‘é‡ |
| é›†æˆæ£€ç´¢å™¨ | 300MB | 150MB | ç»„åˆæ£€ç´¢å™¨ |

### å¯æ‰©å±•æ€§

- **æ–‡æ¡£æ£€ç´¢å™¨**: é«˜æ•ˆæ‰©å±•åˆ°100K+æ–‡æ¡£
- **å‘é‡æ£€ç´¢å™¨**: å—å‘é‡å­˜å‚¨åç«¯é™åˆ¶
- **é›†æˆæ£€ç´¢å™¨**: éšå„ä¸ªæ£€ç´¢å™¨é™åˆ¶æ‰©å±•

## ğŸ¯ æœªæ¥å¢å¼º

### è®¡åˆ’åŠŸèƒ½

1. **é«˜çº§æ£€ç´¢ç®—æ³•**
   - ColBERTé£æ ¼çš„åæœŸäº¤äº’
   - å¯†é›†æ®µè½æ£€ç´¢ï¼ˆDPRï¼‰
   - åˆ†å±‚æ£€ç´¢ç­–ç•¥

2. **æ€§èƒ½ä¼˜åŒ–**
   - å‘é‡æ“ä½œçš„GPUåŠ é€Ÿ
   - å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼æ£€ç´¢
   - å¸¦Redisåç«¯çš„é«˜çº§ç¼“å­˜

3. **é›†æˆåŠŸèƒ½**
   - æ›´å¤šLLMæä¾›å•†é›†æˆ
   - æµå¼å“åº”æ”¯æŒ
   - å·¥å…·è°ƒç”¨å’Œå‡½æ•°æ‰§è¡Œ

4. **ç›‘æ§å’Œåˆ†æ**
   - è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
   - æ£€ç´¢è´¨é‡åˆ†æ
   - A/Bæµ‹è¯•æ¡†æ¶

### æ¶æ„æ¼”è¿›

æ¶æ„è®¾è®¡ä¸ºå¯ä»¥éšä»¥ä¸‹æ–¹é¢æ¼”è¿›ï¼š

- **æ’ä»¶ç³»ç»Ÿ**: åŠ¨æ€ç»„ä»¶åŠ è½½
- **é…ç½®ç®¡ç†**: åŸºäºç¯å¢ƒçš„é…ç½®
- **å¯è§‚æµ‹æ€§**: å…¨é¢çš„æ—¥å¿—å’ŒæŒ‡æ ‡
- **å¯æ‰©å±•æ€§**: æ°´å¹³æ‰©å±•èƒ½åŠ›

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§[LICENSE](LICENSE)æ–‡ä»¶ã€‚

## ğŸ¤ è‡´è°¢

- **LangChainç¤¾åŒº**: æä¾›çµæ„Ÿå’Œæ¶æ„æ¨¡å¼
- **ä¿¡æ¯æ£€ç´¢ç ”ç©¶**: æä¾›åº•å±‚ç®—æ³•å’ŒæŠ€æœ¯
- **å¼€æºè´¡çŒ®è€…**: æä¾›ä½¿æ­¤é¡¹ç›®æˆä¸ºå¯èƒ½çš„å·¥å…·å’Œåº“

---

**âš¡ ä¸ºLLMåº”ç”¨å¼€å‘å’Œæ•™å­¦å“è¶Šè€Œæ„å»º**